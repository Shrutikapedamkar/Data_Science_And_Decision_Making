# -*- coding: utf-8 -*-
"""Heart Failure Prediction_Revised.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FL5sS35rYes9hH4_Jt7klS6sf3HsbyKi

# **Heart Failure Prediction**

Context
Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.

People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.

Attribute Information
1. Age: age of the patient [years]
2. Sex: sex of the patient [M: Male, F: Female]
3. ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina,   NAP: Non-Anginal Pain, ASY: Asymptomatic]
4. RestingBP: resting blood pressure [mm Hg]
5. Cholesterol: serum cholesterol [mm/dl]
6. FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]
9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
10. Oldpeak: oldpeak = ST [Numeric value measured in depression]
11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
12. HeartDisease: output class [1: heart disease, 0: Normal]


Source
This dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.datasets import make_imbalance
from collections import Counter
import sklearn
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('https://raw.githubusercontent.com/Shrutikapedamkar/DataScience_Assignment1_Imbalance-Dataset/main/heart.csv')
df

"""# **Load, Inspect and Clean the dataset**"""

#first 5 rows of the dataset
df.head()

#last 5 rows of the dataset
df.tail()

#prints information about the dataset
df.info()

#datatype of all the columns of the dataset
df.dtypes

#Calculate statistical data of the dataset
df.describe()

#Print all the columns in the dataset
df.columns

# Print the shape of the database
df.shape

#HISTOGRAM
df.hist(figsize=(20,10))
plt.show()

# HEATMAP.
plt.figure(figsize=(20,10))
hmap= df.corr()
sns.heatmap(hmap,cmap="RdYlGn",annot=True)
hmap

#count number of attributes in each class
df.HeartDisease.value_counts()

#count number of null values in each column
df.isnull().sum(axis=0)

#onehotencoder = OneHotEncoder()
onehotencoder = pd.get_dummies(df, columns = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'])
print(onehotencoder)

#Copying data of the target attribute in y
y = onehotencoder['HeartDisease'].copy()
y

#Copying data of all the attributes except the target attribute in x
x = onehotencoder.drop(['HeartDisease'], axis=1)
x

#Histogram of dataset before imbalancing the dataset
histogram = onehotencoder['HeartDisease'].hist()
plt.title('Dataset before imbalancing')
plt.show()

def imbalance_dataset(x, y, imbalance_percentage):
  imbalance_percentage = imbalance_percentage - 50
  yes = int(len(onehotencoder[onehotencoder['HeartDisease'] == 1]))
  no = len(onehotencoder[onehotencoder['HeartDisease'] == 0]) - (int(len(onehotencoder[onehotencoder['HeartDisease'] == 0]) * (imbalance_percentage /100)))
  x_imb, y_imb = make_imbalance(x, y, sampling_strategy = {1: yes, 0: no}, random_state =1)
  return y_imb

"""## Low Imbalance (65%)"""

df_65 = imbalance_dataset(x,y,65)
print(f'Distribution after imbalancing: {Counter(df_65)}')
#Copying data of all the attributes except the target attribute in x
x1 = df_65.drop(['HeartDisease'], axis=1)
#Copying data of the target attribute in y
y1 = df_65['HeartDisease'].copy()

#Histogram of low imbalance dataset
histogram = df_65.hist()
plt.title('Low imbalance dataset')
plt.show()

"""# Medium Imbalance (75%)"""

df_75 = imbalance_dataset(x,y,75)
print(f'Distribution after imbalancing: {Counter(df_75)}')
#Copying data of all the attributes except the target attribute in x
x2 = df_75.drop(['HeartDisease'], axis=1)
#Copying data of the target attribute in y
y2 = df_75['HeartDisease'].copy()

#Histogram of medium imbalance dataset
histogram = df_75.hist()
plt.title('Medium imbalance dataset')
plt.show()

"""# High Imbalance (90%)"""

df_90 = imbalance_dataset(x,y,90)
print(f'Distribution after imbalancing: {Counter(df_90)}')
#Copying data of all the attributes except the target attribute in x
x3 = df_90.drop(['HeartDisease'], axis=1)
#Copying data of the target attribute in y
y3 = df_90['HeartDisease'].copy()

#Histogram of high imbalance dataset
histogram = df_90.hist()
plt.title('High imbalance dataset')
plt.show()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=50, stratify=y)
print(len(x_train),len(x_test),len(y_train),len(y_test))

x1_train, x1_test, y1_train, y1_test = train_test_split(x, y, test_size=0.20, random_state=50, stratify=y)
print(len(x_train),len(x_test),len(y_train),len(y_test))

rf =RandomForestClassifier(n_estimators=100, max_depth=3)
#rf.fit(x1_train, y1_train)
stratified_kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)
lst_stratified_accuracy =[]

stratified_kf.split(x,y)

for train_index, test_index in stratified_kf.split(x,y):
  x1_train_fold, x1_test_fold = x.iloc[train_index], x.iloc[test_index]
  y1_train_fold, y1_test_fold = y.iloc[train_index], y.iloc[test_index]
  rf.fit(x1_train_fold,y1_train_fold )
  lst_stratified_accuracy.append(rf.score(x1_test_fold,y1_test_fold))

scores = cross_val_score(rf, x1_train_fold, y1_train_fold, cv =10)
print("scores: .%2f +/- .%2f" % (scores.mean(), scores.std()))
print("============================================")
print(sorted(sklearn.metrics.SCORERS.keys()))
print("============================================")
print(classification_report(y_test, rf.predict(x_test)))

results = confusion_matrix(y_test,  rf.predict(x_test))
print(results)

plot_confusion_matrix(rf, x_test,  (y_test))

"""Create 10 stratified folds (to ensure the imbalance ratio remains the same in each fold) for each of the datasets."""

stratified_kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)

# Use the elbow method to select the number of clusters in your data using 3 PCs.
def std_pca(df, var=0.5):
    y_flag = False
    if 'y' in df:
      y = df['y']
      df = df.loc[:, df. columns != 'y']
      y_flag = True
    standard_transformer = StandardScaler()
    df = standard_transformer.fit_transform(x)
    pca = PCA(n_components=var, random_state=100)
    x_pca = pca.fit_transform(df)
    if y_flag:
      x_pca = np.column_stack((x_pca, y))
    return x_pca, pca


#pca.explained_variance_ratio_.cumsum()

"""#1. Using the Elbow method and the Silhouette method, identify the number of clusters in the dataset.
#There should be some level of agreement between these indices (or at least you should be able to identify lower and upper bounds).
#2. Run k-means in the data set using the identified number of clusters. Select as final clustering that with the lowest output criteria
"""

for train_index, test_index in stratified_kf.split(x,y):
  x1_train_fold, x1_test_fold = x.iloc[train_index], x.iloc[test_index]
  y1_train_fold, y1_test_fold = y.iloc[train_index], y.iloc[test_index]
  x1_train_fold = pd.concat([x1_train_fold,y1_train_fold])
  df_x1Fold, pca = std_pca(x1_train_fold, var=2)
  list_initial = []
  list_sil = []
  for k in range(1, 9):
      k_means = KMeans(n_clusters = k)
      y_pred = k_means.fit_predict(df_x1Fold)
      list_initial.append(k_means.inertia_)
      
      if k > 1:
        list_sil.append(silhouette_score(df_x1Fold, y_pred))

  plt.figure(figsize=(5,5))
  plt.plot(range(1, 9), list_initial, 'o-')
  plt.xlabel('k')
  plt.ylabel('kmean_inertia')
  plt.show()

# Now use the silhouette score to choose between the two candidate k values from the previous cell
plt.figure(figsize=(8,5))
plt.bar(range(2, 9), list_sil)
plt.xlabel('k')
plt.ylabel('silhouette_score')
plt.show()

# predict model
for train_index, test_index in stratified_kf.split(x,y):
  x1_train_fold, x1_test_fold = x.iloc[train_index], x.iloc[test_index]
  y1_train_fold, y1_test_fold = y.iloc[train_index], y.iloc[test_index]
  x1_train_fold = pd.concat([x1_train_fold,y1_train_fold])
  df_x1Fold, pca = std_pca(x1_train_fold, var=2)
  k_means = KMeans(n_clusters=2)
  y_pred = k_means.fit_predict(df_x1Fold)
  plt.scatter(df_x1Fold[y_pred == 0, 0], df_x1Fold[y_pred == 0, 1], s = 100, c = 'red', label = 'group_one')
  plt.scatter(df_x1Fold[y_pred == 1, 0], df_x1Fold[y_pred == 1, 1], s = 100, c = 'green', label = 'group_two')
  plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
  
  list_centroid = []
  list_minorityclass = []
  list_centroid.append(k_means.cluster_centers_)
  # print(k_means.cluster_centers_[:, 0])
  # print(k_means.cluster_centers_[:, 1])
  # print(len(df_x1Fold[y_pred == 0, 0]))
  # print(len(df_x1Fold[y_pred == 0, 1]))

  # print(len(df_x1Fold[y_pred == 1, 0]))
  # print(len(df_x1Fold[y_pred == 1, 1]))
  # print(df_x1Fold[y_pred == 1, 0])
  # print(df_x1Fold[y_pred == 0].values.count())
  test1 = pd.Index(df_x1Fold[y_pred == 0, 0]).value_counts()
  test2 = pd.Index(df_x1Fold[y_pred == 0, 1]).value_counts()
  test3 = pd.Index(df_x1Fold[y_pred == 1, 0]).value_counts()
  test4 = pd.Index(df_x1Fold[y_pred == 1,1]).value_counts()


  print(len(test1))
  print(len(test2))
  print(len(test3))
  print(len(test4))

  #print(len(test[test == 1]))
  # test_one = [i for i in y_pred if i == 0]
  # test_two = [i for i in y_pred if i == 1]

  # print(len(test_one),len(test_two))

  # print(df_x1Fold[y_pred == 1, 0].values.count())



  # print(len(k_means.cluster_centers_[:,1]))



  plt.title('Clusters of df_x1Fold')
  plt.xlabel('x')
  plt.ylabel('y')
  plt.legend()
  plt.show()

