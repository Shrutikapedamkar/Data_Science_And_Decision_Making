# -*- coding: utf-8 -*-
"""South Asian Churn_Revised.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ve8tV93D2OLGPV8fBoNk-B9jZpUC_arv

## **South Asian Churn Dataset**
Context
The SATO data set used is real life data collected from a major wireless telecom operator in South Asia.

Content



1. Aggregate of Total Revenue: The overall monthly revenue earned in Rupees by the carrier in the months August & September 2015.
2. Aggregate of SMS Revenue: The revenue earned through the SMS service used by the subscriber.
3. Aggregate of Data Revenue: The revenue earned through the Data service used by the subscriber.
4. Aggregate of Off Net Revenue: The revenue earned by the calls etc. made to the off-network (not the same network as the subscriber) customers by the carrier’s present subscriber.
5. Aggregate of On Net Revenue: The revenue earned by the calls etc. made to the on-network (on the same network as the subscriber) customers by the carrier’s present subscriber.
6. Network Age: The time passed since the subscriber started using the services of the carrier.
7. User Type: This detail helps in knowing if the user is subscribed to a 2G or 3G service.
8. Aggregate of Complaint Count: The number of complaints made by the subscribers.
9. Favorite Other Network: This information can certainly have a huge impact on churn ratio as it gives the information about which other network or operator the subscribers makes the most of the calls to and thus might influence the customer to move to that network to save money.
10. Aggregate of Data Volume: The volume of the data service used by the subscriber.

South Asian Telecom Operator (SATO) data set is a real life data collected from a major wireless telecom operator in South Asia. Most of the attributes in the data sets are associated with call detail records (CDR), billing and personal information. It contains 2000 subscribers. All of these subscribers were not contract based and had a monthly based subscription. The subscriber data was extracted from the time interval of two months i.e. August and September 2015.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.datasets import make_imbalance
from collections import Counter
import sklearn
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('https://raw.githubusercontent.com/Shrutikapedamkar/DataScience_Assignment1_Imbalance-Dataset/main/South%20Asian%20Wireless%20Telecom%20Operator%20(SATO%202015).csv')
df

"""# **Load Inspect and clean dataset**"""

#first 5 rows of the dataset
df.head()

#last 5 rows of the dataset
df.tail()

#prints information about the dataset
df.info()

#datatype of all the columns of the dataset
df.dtypes

#Calculate statistical data of the dataset
df.describe()

#Print all the columns in the dataset
df.columns

# Print the shape of the database
df.shape

#HISTOGRAM
df.hist(figsize=(20,10))
plt.show()

# HEATMAP.
plt.figure(figsize=(20,10))
hmap= df.corr()
sns.heatmap(hmap,cmap="RdYlGn",annot=True)
hmap

#count number of attributes in each class
df.Class.value_counts()

#count number of null values in each column
df.isnull().sum(axis=0)

#There are null values in aug_user_type, sep_user_type, aug_fav_a, sep_fav_a. Hence we replace the missing values with mean
df =df.fillna(df.mode().iloc[0])
df.isnull().sum(axis=0)

df.info()

#Removing Outliers
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

df = df[~((df < (Q1-1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]
df.shape

#onehotencoder = OneHotEncoder()
onehotencoder = pd.get_dummies(df, columns = ['aug_user_type','sep_user_type','aug_fav_a','sep_fav_a'])
print(onehotencoder)

#Label Encoder to convert categorical label to numerical
labelencoder =  LabelEncoder()
onehotencoder["Y"] = labelencoder.fit_transform(onehotencoder["Class"])
onehotencoder

#Copying data of the target attribute in y
y = onehotencoder['Y'].copy()
y

#Copying data of all the attributes except the target attribute in x
x = onehotencoder.drop(['Y'], axis=1)
x

#Histogram of dataset before imbalancing the dataset
histogram = onehotencoder['Y'].hist()
plt.title('Dataset before imbalancing')
plt.show()

"""After removing outliers, dataset became imbalance. HEnce we first balance the dataset"""

from imblearn.over_sampling import RandomOverSampler
over_sampler = RandomOverSampler(random_state=42)
x, y = over_sampler.fit_resample(x, y)

#Histogram of dataset before imbalancing the dataset
histogram = y.hist()
plt.title('Dataset before imbalancing')
plt.show()

def imbalance_dataset(x, y, imbalance_percentage):
  imbalance_percentage = imbalance_percentage - 50
  yes = int(len(df[df['Class'] == 'Active']))
  no = len(df[df['Class'] == 'Churned']) - (int(len(df[df['Class'] == 'Churned']) * (imbalance_percentage /100)))
  x_imb, y_imb = make_imbalance(x, y, sampling_strategy = {'Active': yes, 'Churned': no}, random_state =1)
  return y_imb

y.shape

"""# Low Imbalance (65%)"""

x1, y1 =make_imbalance(x, y, sampling_strategy = {1: 653, 0: 553}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y1)}')

#Histogram of low imbalance dataset
histogram = y1.hist()
plt.title('Low imbalance dataset')
plt.show()

"""# Medium Balanced (75%)"""



x2, y2 =make_imbalance(x, y, sampling_strategy = {1: 653, 0: 400}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y2)}')

#Histogram of medium imbalance dataset
histogram = y2.hist()
plt.title('Medium imbalance dataset')
plt.show()

"""# High Imbalance (90%)"""

x3, y3 =make_imbalance(x, y, sampling_strategy = {1: 653, 0: 72}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y3)}')

#Histogram of high imbalance dataset
histogram = y3.hist()
plt.title('High imbalance dataset')
plt.show()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=50, stratify=y)
print(len(x_train),len(x_test),len(y_train),len(y_test))

"""# To establish a baseline, perform stratified cross-validation on each of the datasets and their surrogates and train a random forest. Report baseline results using appropriate metrics."""

rf =RandomForestClassifier(n_estimators=100, max_depth=3)
#rf.fit(x1_train, y1_train)
stratified_kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)
lst_stratified_accuracy =[]

stratified_kf.split(x1,y1)

for train_index, test_index in stratified_kf.split(x,y):
  x_train_fold, x_test_fold = x.iloc[train_index], x.iloc[test_index]
  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]
  rf.fit(x_train_fold,y_train_fold )
  lst_stratified_accuracy.append(rf.score(x_test_fold,y_test_fold))

