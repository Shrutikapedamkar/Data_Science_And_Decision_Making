# -*- coding: utf-8 -*-
"""Balanced_Bank_Revised.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12AzVMs9KiKl5U-jCfekgIwLsMi2dVo8h
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.datasets import make_imbalance
from collections import Counter
import sklearn
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('https://raw.githubusercontent.com/Shrutikapedamkar/DataScience_Assignment1_Imbalance-Dataset/main/balanced_bank.csv')
df

"""# **Load, Inspect and Clean the dataset**"""

#first 5 rows of the dataset
df.head()

#last 5 rows of the dataset
df.tail()

#prints information about the dataset
df.info()

#datatype of all the columns of the dataset
df.dtypes

#Calculate statistical data of the dataset
df.describe()

#Print all the columns in the dataset
df.columns

# Print the shape of the database
df.shape

#HISTOGRAM
df.hist(figsize=(20,10))
plt.show()

# HEATMAP.
plt.figure(figsize=(20,10))
hmap= df.corr()
sns.heatmap(hmap,cmap="RdYlGn",annot=True)
hmap

#count number of attributes in each class
df.y.value_counts()



#count number of null values in each column
df.isnull().sum(axis=0)

labelencoder =  LabelEncoder()
df["Y"] = labelencoder.fit_transform(df["y"])
df

#onehotencoder = OneHotEncoder()
onehotencoder = pd.get_dummies(df, columns = ['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome'])
print(onehotencoder)

#Copying data of the target attribute in y
y = onehotencoder['Y'].copy()
y

#Copying data of all the attributes except the target attribute in x
x = onehotencoder.drop(['y','Y'], axis=1)
x

#Histogram of dataset before imbalancing the dataset
histogram = onehotencoder['Y'].hist()
plt.title('Dataset before imbalancing')
plt.show()

def imbalance_dataset(x, y, imbalance_percentage):
  imbalance_percentage = imbalance_percentage - 50
  yes = int(len(df[df['y'] == 'yes']))
  no = len(df[df['y'] == 'no']) - (int(len(df[df['y'] == 'no']) * (imbalance_percentage /100)))
  x_imb, y_imb = make_imbalance(x, y, sampling_strategy = {'yes': yes, 'no': no}, random_state =1)
  return y_imb

"""# Low Imbalance (65%)"""

x1, y1 =make_imbalance(x, y, sampling_strategy = {1: 4640, 0: 1624}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y1)}')

#Histogram of low imbalance dataset
histogram = y1.hist()
plt.title('Low imbalance dataset')
plt.show()

"""# Medium Imbalance (75%)"""

x2, y2 =make_imbalance(x, y, sampling_strategy = {1: 4640, 0: 1160}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y2)}')

#Histogram of medium imbalance dataset
histogram = y2.hist()
plt.title('Medium imbalance dataset')
plt.show()

"""# High Imbalance (90%)"""

x3, y3 =make_imbalance(x, y, sampling_strategy = {1: 4640, 0: 470}, random_state =42)
print(f'Distribution after imbalancing: {Counter(y3)}')

#Histogram of high imbalance dataset
histogram = y3.hist()
plt.title('High imbalance dataset')
plt.show()

x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.20, random_state=50, stratify=y1)
print(len(x1_train),len(x1_test),len(y1_train),len(y1_test))

x1_train

"""# To establish a baseline, perform stratified cross-validation on each of the datasets and their surrogates and train a random forest. Report baseline results using appropriate metrics."""

rf =RandomForestClassifier(n_estimators=100, max_depth=3)
#rf.fit(x1_train, y1_train)
stratified_kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)
lst_stratified_accuracy =[]

stratified_kf.split(x1,y1)

for train_index, test_index in stratified_kf.split(x1,y1):
  x1_train_fold, x1_test_fold = x1.iloc[train_index], x1.iloc[test_index]
  y1_train_fold, y1_test_fold = y1.iloc[train_index], y1.iloc[test_index]
  rf.fit(x1_train_fold,y1_train_fold )
  lst_stratified_accuracy.append(rf.score(x1_test_fold,y1_test_fold))

scores = cross_val_score(rf, x1_train_fold, y1_train_fold, cv =10)
print("scores: .%2f +/- .%2f" % (scores.mean(), scores.std()))
print("============================================")
print(sorted(sklearn.metrics.SCORERS.keys()))
print("============================================")
print(classification_report(y1_test, rf.predict(x1_test)))

results = confusion_matrix(y1_test,  rf.predict(x1_test))
print(results)

plot_confusion_matrix(rf, x1_test,  (y1_test))

"""# Create 10 stratified folds (to ensure the imbalance ratio remains the same in each fold) for each of the datasets."""

stratified_kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 1)

# Use the elbow method to select the number of clusters in your data using 3 PCs.
def std_pca(df, var=0.5):
    y_flag = False
    if 'y' in df:
      y = df['y']
      df = df.loc[:, df. columns != 'y']
      y_flag = True
    standard_transformer = StandardScaler()
    df = standard_transformer.fit_transform(x1)
    pca = PCA(n_components=var, random_state=100)
    x_pca = pca.fit_transform(df)
    if y_flag:
      x_pca = np.column_stack((x_pca, y))
    return x_pca, pca


#pca.explained_variance_ratio_.cumsum()

"""#1. Using the Elbow method and the Silhouette method, identify the number of clusters in the dataset.
#There should be some level of agreement between these indices (or at least you should be able to identify lower and upper bounds).
#2. Run k-means in the data set using the identified number of clusters. Select as final clustering that with the lowest output criteria
"""

for train_index, test_index in stratified_kf.split(x1,y1):
  x1_train_fold, x1_test_fold = x1.iloc[train_index], x1.iloc[test_index]
  y1_train_fold, y1_test_fold = y1.iloc[train_index], y1.iloc[test_index]
  x1_train_fold = pd.concat([x1_train_fold,y1_train_fold])
  df_x1Fold, pca = std_pca(x1_train_fold, var=2)
  list_initial = []
  list_sil = []
  for k in range(1, 9):
      k_means = KMeans(n_clusters = k)
      y_pred = k_means.fit_predict(df_x1Fold)
      list_initial.append(k_means.inertia_)
      
      if k > 1:
        list_sil.append(silhouette_score(df_x1Fold, y_pred))

  plt.figure(figsize=(5,5))
  plt.plot(range(1, 9), list_initial, 'o-')
  plt.xlabel('k')
  plt.ylabel('kmean_inertia')
  plt.show()

# Now use the silhouette score to choose between the two candidate k values from the previous cell
plt.figure(figsize=(8,5))
plt.bar(range(2, 9), list_sil)
plt.xlabel('k')
plt.ylabel('silhouette_score')
plt.show()

# predict model
for train_index, test_index in stratified_kf.split(x1,y1):
  x1_train_fold, x1_test_fold = x1.iloc[train_index], x1.iloc[test_index]
  y1_train_fold, y1_test_fold = y1.iloc[train_index], y1.iloc[test_index]
  x1_train_fold = pd.concat([x1_train_fold,y1_train_fold])
  df_x1Fold, pca = std_pca(x1_train_fold, var=2)
  k_means = KMeans(n_clusters=2)
  y_pred = k_means.fit_predict(df_x1Fold)
  # print(df_x1Fold[y_pred == 0].shape, df_x1Fold[y_pred == 0, 1].shape)
  # print(df_x1Fold[y_pred == 0])
  # print(df_x1Fold[y_pred == 0, 1])
  plt.scatter(df_x1Fold[y_pred == 0, 0], df_x1Fold[y_pred == 0, 1], s = 100, c = 'red', label = 'group_one')
  plt.scatter(df_x1Fold[y_pred == 1, 0], df_x1Fold[y_pred == 1, 1], s = 100, c = 'green', label = 'group_two')
  plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
  
  list_centroid = []
  list_minorityclass = []
  list_centroid.append(k_means.cluster_centers_)
  print(k_means.cluster_centers_)

  plt.title('Clusters of df_x1Fold')
  plt.xlabel('x')
  plt.ylabel('y')
  plt.legend()
  plt.show()

list_centroid

